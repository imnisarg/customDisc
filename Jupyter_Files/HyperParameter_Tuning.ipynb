{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning for predicting Total Discount\n",
    "\n",
    "Having identified that GBR model works well in previous section based on K fold cross validation scores and Training mean squared error , in this notebook we try to tune the model for better performance\n",
    "\n",
    "We use bayesSearchCV to find the best parameters for our GBR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from types import FunctionType\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Importing sklearn methods\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "\n",
    "import skopt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3232645073885446\n"
     ]
    }
   ],
   "source": [
    "# data2.xlsx is the data obtained after running the feature engineering code\n",
    "# data3.csv : converted data2.xlsx to data3.csv because of easiness of use of csv files in webapps\n",
    "path = r\"C:\\Users\\NISARG\\Desktop\\mech\\Finance\\Maverick\\CODE\"   #change the path to your local path\n",
    "df = pd.read_csv(path + \"\\data3.csv\")\n",
    "\n",
    "\n",
    "count = 0\n",
    "index_test = list()\n",
    "index_train = list()\n",
    "for i in range(len(df['upper_limit'])):\n",
    "    if(df['Discount_Total'][i]>df['upper_limit'][i]):\n",
    "        count+=1\n",
    "        index_test.append(i)\n",
    "    else:\n",
    "        index_train.append(i)\n",
    "print(count/len(df['upper_limit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Here we get the train and test datasets based on the logic described above\n",
    "'''\n",
    "\n",
    "df_test = df.iloc[index_test]\n",
    "df_test = df_test.reset_index()\n",
    "df_train = df.iloc[index_train]\n",
    "df_train = df_train.reset_index()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding categorical variables here\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode(highGTOData):\n",
    "    lb_make = LabelEncoder()\n",
    "    highGTOData['sdfc_Tier'] = lb_make.fit_transform(highGTOData['sdfc_Tier'])\n",
    "    for i in range(len(highGTOData['GTO_2019'])):\n",
    "        if(highGTOData['poc_image'][i]==0):\n",
    "            highGTOData['poc_image'][i] = \"Mainstream\"\n",
    "    highGTOData['poc_image'] = lb_make.fit_transform(highGTOData['poc_image'])\n",
    "    highGTOData['segment'] = lb_make.fit_transform(highGTOData['segment'])\n",
    "    highGTOData['sub_segment'] = lb_make.fit_transform(highGTOData['sub_segment'])\n",
    "    highGTOData['Product Set'] = lb_make.fit_transform(highGTOData['Product Set'])\n",
    "    highGTOData['Brand'] = lb_make.fit_transform(highGTOData['Brand'])\n",
    "    highGTOData['Sub-Brand'] = lb_make.fit_transform(highGTOData['Sub-Brand'])\n",
    "    highGTOData['Pack_Type'] = lb_make.fit_transform(highGTOData['Pack_Type'])\n",
    "    highGTOData['Returnalility'] = lb_make.fit_transform(highGTOData['Returnalility'])\n",
    "    highGTOData['province'] = lb_make.fit_transform(highGTOData['province'])\n",
    "    highGTOData['GTO_growth'] = highGTOData['Expected_GTO'] - highGTOData['GTO_2019']\n",
    "    return highGTOData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NISARG\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "df_test = encode(df_test)\n",
    "df_train = encode(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "lowGTOData_train = df_train[df_train.GTO_2019<10000]\n",
    "lowGTOData_test = df_test[df_test.GTO_2019<10000]\n",
    "\n",
    "midGTOData_train = df_train[(df_train.GTO_2019>10000)&(df_train.GTO_2019<50000)]\n",
    "midGTOData_test = df_test[(df_test.GTO_2019>10000)&(df_test.GTO_2019<50000)]\n",
    "\n",
    "highGTOData_train = df_train[df_train.GTO_2019>50000]\n",
    "highGTOData_test = df_test[df_test.GTO_2019>50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keeping the essential columns as seen from the Exploratory Data Analysis\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "lowGTOData_train = lowGTOData_train.reset_index()\n",
    "lowGTOData_test = lowGTOData_test.reset_index()\n",
    "target_train = lowGTOData_train['Discount_Total']\n",
    "target_test = lowGTOData_test['Discount_Total']\n",
    "colsToKeep = ['Volume_2019' , 'Volume_2018'  , 'Expected_GTO'  , 'Expected_product_volume', 'profitability_indicator' , 'upper_limit'  ,'sdfc_Tier'  , 'loyalty_index' , 'Returnalility', 'market_cap' ]\n",
    "features_train = lowGTOData_train[colsToKeep]\n",
    "features_test = lowGTOData_test[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to focus on in Gradient Boosting Regressor\n",
    "\n",
    "    1) loss :\n",
    "        a. ls : least square regression\n",
    "        b. lad : least absolute deviation\n",
    "        c. huber : Combination of ls & lad\n",
    "        d. quantile : Used in quantile regression\n",
    "        \n",
    "    2) learning_rate : Learning rate is used to shrink the contribution of each tree in GBR , we try values of learning rate from 0.05 to 0.3 with step of 0.05\n",
    "    \n",
    "    3) n_estimators : No of boostings performed . Larger the number , better is the fit , but it can lead to overfitting as well\n",
    "    \n",
    "    4) subsample : The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "    \n",
    "    5) criterion : function to measure quality of split\n",
    "        a. friedman_mse : mse with improvement by friedman\n",
    "        b. mse : mean squared error\n",
    "        \n",
    "    6) max_depth : Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. The best value depends on the interaction of the input variables. We try max_depth from 2 to 8 with step of 1\n",
    "    \n",
    "    7) max_features : auto , sqrt , log2 \n",
    "        a. auto : max_features = n_features\n",
    "        b. sqrt : max_features = sqrt(n_features)\n",
    "        c. log2 : max_features = log2(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['loss'] = ['ls', 'lad', 'huber', 'quantile']\n",
    "params['learning_rate'] = [0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "params['n_estimators'] = [50,100,150,200,250,300,350,400]\n",
    "params['subsample'] = [0.6,0.8,1]\n",
    "params['criterion'] = ['friedman_mse', 'mse']\n",
    "params['max_depth'] = [2,3,4,5,6,7,8]\n",
    "params['max_features'] = ['auto', 'sqrt', 'log2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# define the search\n",
    "search = BayesSearchCV(estimator=GradientBoostingRegressor(), search_spaces=params, n_jobs=-1, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7860501247020013\n",
      "OrderedDict([('criterion', 'friedman_mse'), ('learning_rate', 0.05), ('loss', 'huber'), ('max_depth', 4), ('max_features', 'sqrt'), ('n_estimators', 400), ('subsample', 0.8)])\n"
     ]
    }
   ],
   "source": [
    "search.fit(features_train,target_train)\n",
    "# report the best result\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Parameters for lowGTO Data\n",
    "    1) Criteria = friedman_mse\n",
    "\n",
    "    2) learning_rate = 0.05\n",
    "\n",
    "    3) loss = huber\n",
    "\n",
    "    4) max_depth = 4\n",
    "\n",
    "    5) max_features = sqrt\n",
    "\n",
    "    6) n_estimators = 400\n",
    "\n",
    "    7) subsample = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ML Model with best parameters and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72989008 0.68806548 0.88944078 0.66727193 0.78155321 0.89647984\n",
      " 0.93695627 0.71685247 0.72706194 0.71846468]\n",
      "0.7752036673578562\n",
      "134.90748179073378\n",
      "822.7794579889197\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(criterion = 'friedman_mse' , n_estimators=400, learning_rate=0.05, max_depth=4, max_features = 'sqrt', loss='huber' , subsample = 0.8)\n",
    "\n",
    "gbr.fit(features_train,target_train)\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=34234)\n",
    "cross_val_scores = cross_val_score(gbr, features_train, target_train, cv=kfold )\n",
    "mse_train = math.sqrt(mean_squared_error(target_train,gbr.predict(features_train)))\n",
    "mse = math.sqrt(mean_squared_error(target_test, gbr.predict(features_test)))\n",
    "print(cross_val_scores)\n",
    "print(np.mean(cross_val_scores))\n",
    "print(mse_train)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "filename = 'lowGTOModel_TotalDiscount.sav'\n",
    "joblib.dump(gbr, open(filename, 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keeping the features as seen from EXPLORATORY DATA ANALYSIS\n",
    "'''\n",
    "\n",
    "midGTOData_train = midGTOData_train.reset_index()\n",
    "midGTOData_test = midGTOData_test.reset_index()\n",
    "target_train = midGTOData_train['Discount_Total']\n",
    "target_test = midGTOData_test['Discount_Total']\n",
    "colsToKeep = ['Volume_2019' , 'Volume_2018' ,'Volume_2019 Product' ,'Expected_GTO','Expected_product_volume' , 'profitability_indicator' , 'upper_limit'  ,'sdfc_Tier'  , 'loyalty_index' , 'Returnalility',  'inventory_lingering_factor', 'market_cap',\n",
    "       'order_size']\n",
    "features_train = midGTOData_train[colsToKeep]\n",
    "features_test = midGTOData_test[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# define the search\n",
    "search = BayesSearchCV(estimator=GradientBoostingRegressor(), search_spaces=params, n_jobs=-1, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49342351243431615\n",
      "OrderedDict([('criterion', 'mse'), ('learning_rate', 0.1), ('loss', 'ls'), ('max_depth', 3), ('max_features', 'auto'), ('n_estimators', 100), ('subsample', 0.8)])\n"
     ]
    }
   ],
   "source": [
    "search.fit(features_train,target_train)\n",
    "# report the best result\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Parameters for Mid GTO Data\n",
    "    1) criterion : mse\n",
    "\n",
    "    2) learning_rate : 0.1\n",
    "\n",
    "    3) loss : ls\n",
    "\n",
    "    4) max_depth = 3\n",
    "\n",
    "    5) n_estimators = 100\n",
    "\n",
    "    6) subsample = 0.8\n",
    "    \n",
    "    7) max_features = auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model on best parameters and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(criterion = 'mse' , n_estimators=100, learning_rate=0.1, max_depth=3, max_features = 'auto', loss='ls' , subsample = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63638215 0.26322742 0.53102371 0.28744227 0.40290535 0.65835448\n",
      " 0.57073938 0.39541201 0.48698544 0.54880762]\n",
      "0.47812798331009193\n",
      "1458.267350065082\n",
      "6292.354418738532\n"
     ]
    }
   ],
   "source": [
    "gbr.fit(features_train,target_train)\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=34234)\n",
    "cross_val_scores = cross_val_score(gbr, features_train, target_train, cv=kfold )\n",
    "mse_train = math.sqrt(mean_squared_error(target_train,gbr.predict(features_train)))\n",
    "mse = math.sqrt(mean_squared_error(target_test, gbr.predict(features_test)))\n",
    "print(cross_val_scores)\n",
    "print(np.mean(cross_val_scores))\n",
    "print(mse_train)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "filename = 'midGTOModel_TotalDiscount.sav'\n",
    "joblib.dump(gbr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keeping the features as seen from EXPLORATORY DATA ANALYSIS\n",
    "'''\n",
    "\n",
    "\n",
    "highGTOData_train = highGTOData_train.reset_index()\n",
    "highGTOData_test = highGTOData_test.reset_index()\n",
    "target_train = highGTOData_train['Discount_Total']\n",
    "target_test = highGTOData_test['Discount_Total']\n",
    "colsToKeep = ['Volume_2019' , 'Volume_2018' ,'Volume_2019 Product' ,'Expected_GTO','Expected_product_volume' , 'profitability_indicator' , 'upper_limit'  ,  'inventory_lingering_factor',\n",
    "       'order_size']\n",
    "features_train = highGTOData_train[colsToKeep]\n",
    "features_test = highGTOData_test[colsToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=34234)\n",
    "# define the search\n",
    "search = BayesSearchCV(estimator=GradientBoostingRegressor(), search_spaces=params, n_jobs=-1, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2789369206161165\n",
      "OrderedDict([('criterion', 'mse'), ('learning_rate', 0.3), ('loss', 'huber'), ('max_depth', 2), ('max_features', 'auto'), ('n_estimators', 200), ('subsample', 0.8)])\n"
     ]
    }
   ],
   "source": [
    "search.fit(features_train,target_train)\n",
    "# report the best result\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Parameters for High GTO Data\n",
    "    1) criterion - mse\n",
    "\n",
    "    2) learning_rate = 0.3\n",
    "\n",
    "    3) loss : huber\n",
    "\n",
    "    4) max_depth : 2\n",
    "\n",
    "    5) max_features - auto\n",
    "\n",
    "    6) n_estimators = 200\n",
    "\n",
    "    7) subsample = 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model based on best parameters and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(criterion = 'mse' , n_estimators=200, learning_rate=0.3, max_depth=2, max_features = 'auto', loss='huber' , subsample = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.10615611  0.41143118  0.94439123  0.63191923  0.63289689 -0.69701788\n",
      "  0.48614344 -0.34885166  0.8316522   0.88129612]\n",
      "-0.1332295360623002\n",
      "776.6290346486898\n",
      "181182.47494710097\n"
     ]
    }
   ],
   "source": [
    "gbr.fit(features_train,target_train)\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=34234)\n",
    "cross_val_scores = cross_val_score(gbr, features_train, target_train, cv=kfold )\n",
    "mse_train = math.sqrt(mean_squared_error(target_train,gbr.predict(features_train)))\n",
    "mse = math.sqrt(mean_squared_error(target_test, gbr.predict(features_test)))\n",
    "print(cross_val_scores)\n",
    "print(np.mean(cross_val_scores))\n",
    "print(mse_train)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "filename = 'HighGTOModel_TotalDiscount.sav'\n",
    "joblib.dump(gbr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
